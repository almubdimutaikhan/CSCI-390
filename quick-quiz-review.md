
---

# ğŸ§­ Topic Map

| Exam Topic                                        | Covered in                        | Core Ideas                                                       |
| ------------------------------------------------- | --------------------------------- | ---------------------------------------------------------------- |
| **Learning Agent**                                | ch 18 (Â§1â€“3)                      | performance element, learning element, critic, problem generator |
| **Ockhamâ€™s Razor**                                | ch 18 p 12 & slides 15 â€“ 16       | simplicity vs consistency                                        |
| **Decision Trees & Pruning**                      | ch 18 p 22 â€“ 31 + part2 (Pruning) | entropy, information gain, Chi-square, early stopping            |
| **Overfitting / Validation**                      | part2 (Overfitting)               | training vs test error, cross-validation                         |
| **Parametric vs Non-parametric (LDA, KDE / GMM)** | not in these, in other slides     | â€”                                                                |
| **HMM / DBN**                                     | Temporal Prob Models              | already done earlier                                             |

---

Below are **10 quiz-style Q&A blocks per topic**, formatted GitHub-friendly.

---

# ğŸ§© Topic 1 â€“ Learning Agent Model

```
1. What are the four components of a learning agent?
â†’ Performance element, Learning element, Critic, Problem generator.

2. Role of performance element?
â†’ Chooses actions based on current percepts and internal state.

3. Role of learning element?
â†’ Improves the performance element using feedback from the critic.

4. Role of critic?
â†’ Provides performance feedback by comparing behavior to a standard.

5. Role of problem generator?
â†’ Proposes exploratory actions to discover new, informative experiences.

6. Difference between supervised and reinforcement feedback?
â†’ Supervised gives correct labels each instance; RL gives occasional rewards.

7. How does learning modify the agent?
â†’ Adjusts decision mechanisms to improve future performance.

8. Why learning useful for system design?
â†’ Lets environment teach the agent instead of hard-coding rules.

9. What dictates the design of the learning element?
â†’ Type of performance element, component to learn, representation, feedback type.

10. Example mapping:
â†’ Utility-based agent learns perceptâ†’action function; feedback = reward signal.
```

---

# ğŸª¶ Topic 2 â€“ Ockhamâ€™s Razor and Inductive Learning

```
1. Quote meaning:
   â€œEntities should not be multiplied unnecessarilyâ€ â†’ prefer simpler models.

2. Why simplicity helps?
   â†’ Reduces risk of overfitting; better generalization.

3. In inductive learning, what is f(x)?
   â†’ Target function mapping inputs to outputs.

4. What is hypothesis h?
   â†’ Learnerâ€™s approximation of f using training data.

5. When is h consistent?
   â†’ When it agrees with all training examples.

6. Relation of Ockhamâ€™s Razor to decision trees?
   â†’ Prefer smaller trees consistent with data.

7. What is biasâ€“variance intuition here?
   â†’ Simpler models have higher bias, lower variance; complex opposite.

8. Give an example of inductive bias.
   â†’ â€œThe simplest consistent hypothesis is best.â€

9. What assumptions real inductive learning simplifies?
   â†’ Deterministic f, observable inputs, given examples, agent wants to learn f.

10. Why curve-fitting demonstrates Ockhamâ€™s Razor?
   â†’ A smoother curve explaining data points is preferred to jagged overfit.
```

---

# ğŸŒ³ Topic 3 â€“ Decision Tree Learning & Information Gain

```
1. Aim:
   â†’ Find the smallest tree consistent with training examples.

2. Recursion base cases:
   â€¢ All examples same class â†’ return class.
   â€¢ No examples â†’ return default.
   â€¢ No attributes â†’ return mode class.

3. Attribute selection criterion?
   â†’ Information Gain = H(parent) âˆ’ Remainder(A).

4. Entropy formula:
   H(p, n) = âˆ’p/(p+n) logâ‚‚(p/(p+n)) âˆ’ n/(p+n) logâ‚‚(n/(p+n))

5. Remainder(A):
   Î£_i (p_i + n_i)/(p+n) Ã— H(p_i, n_i)

6. Choose attribute with smallest Remainder â†’ highest InfoGain.

7. Example: Patrons? vs Type? (Restaurant)
   Patrons? Gain â‰ˆ 0.54 bits > Type? 0 bits â†’ choose Patrons?.

8. What happens with many attributes?
   â†’ Larger hypothesis space â†’ risk of overfitting.

9. Decision tree expressiveness?
   â†’ Can represent any Boolean function (2â½Â²â¿â¾ possible trees).

10. Why prefer compact tree?
   â†’ Simpler â‡’ better generalization (Ockhamâ€™s Razor).
```

---

# âœ‚ï¸ Topic 4 â€“ Decision Tree Pruning (Chi-Square / Validation)

```
1. Purpose of pruning?
   â†’ Reduce overfitting by removing statistically insignificant splits.

2. Two strategies:
   â†’ Post-pruning (generate then prune) vs Early stopping (halt before split).

3. Chi-square test null hypothesis?
   â†’ â€œNo real patternâ€ between attribute and class.

4. Compute Ï‡Â²:
   Ï‡Â² = Î£_k ( (observed_k âˆ’ expected_k)Â² / expected_k )

5. Degrees of freedom (df):
   df = (#positive + #negative classes âˆ’ 1) Ã— (#attribute values âˆ’ 1)

6. Example threshold:
   â†’ df=3 â†’ critical 7.82 at 5%; if Ï‡Â² > 7.82, keep split; else prune.

7. Why not just remove low-gain attributes directly?
   â†’ Combination of low-gain attributes may still classify jointly (e.g., XOR).

8. Effect of noise:
   â†’ Pruning helps by ignoring spurious correlations.

9. What is Early stopping criterion?
   â†’ Stop if split doesnâ€™t improve InfoGain significantly on validation set.

10. Trade-off:
   â†’ Too much pruning = underfit; too little = overfit.
```

---

# ğŸ“ˆ Topic 5 â€“ Overfitting & Model Generalization / Validation

```
1. Define overfitting:
   â†’ Model fits training noise; performs poorly on unseen data.

2. Define underfitting:
   â†’ Model too simple to capture data patterns.

3. Typical symptom:
   â†’ Training accuracy â†‘, test accuracy â†“ as epochs continue.

4. How to visualize?
   â†’ Plot train/test loss vs epochs; test loss bottoms then rises.

5. Cross-validation (k-fold):
   â†’ Split dataset into k parts; train k times each leaving one fold out.

6. Purpose of validation set:
   â†’ Tune hyperparameters and detect overfitting before test phase.

7. How does hypothesis space size affect overfitting?
   â†’ Larger space â†’ higher variance â†’ greater risk.

8. Give numeric example:
   â†’ Acc(train)=99%, Acc(test)=60% â†’ classic overfit.

9. How to combat overfitting?
   â†’ Simplify model, regularization, pruning, or add data.

10. Why Ockhamâ€™s Razor applies again?
   â†’ Simpler model less likely to memorize noise â‡’ better generalization.
```

---

# ğŸ§  Topic 6 â€“ Temporal Models (HMM / DBN Recap)

(Already derived from previous â€œSection Bâ€)

```
1. Markov assumption â†’ P(X_t | X_0:tâˆ’1) = P(X_t | X_{tâˆ’1})
2. Sensor Markov â†’ P(E_t | X_0:t, E_0:tâˆ’1) = P(E_t | X_t)
3. Filtering â†’ update belief with evidence e_t
4. Prediction â†’ project belief forward without new evidence
5. Smoothing â†’ re-estimate past given future data
6. Forward algorithm â†’ Elapse + Observe + Normalize
7. Stationary distribution â†’ Ï€ = Ï€T
8. HMM components â†’ (Ï€â‚, A, B)
9. Viterbi = most likely state sequence
10. DBN = multi-variable generalization of HMM.
```

---


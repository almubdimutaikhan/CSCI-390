


# Temporal Probability Models â€” Q1â€“Q7 (with Answers)

> Scope: Markov chains, stationary distribution, order, stationarity, short numeric transitions, intuition, tiny diagram.

---

## Q1 â€” One-step transition

**Given:**

* P(Sun | Sun) = 0.8
* P(Rain | Sun) = 0.2
* Initial P(Xâ‚) = [1, 0] (100% Sun)

**Task:** Compute P(Xâ‚‚).

**Answer:**

```
P(Xâ‚‚ = Sun)  = 1 * 0.8 = 0.8
P(Xâ‚‚ = Rain) = 1 * 0.2 = 0.2

P(Xâ‚‚) = (0.8, 0.2)
```

---

## Q2 â€” Stationary distribution

**Transitions:**

* P(Sun|Sun)=0.8, P(Rain|Sun)=0.2
* P(Sun|Rain)=0.4, P(Rain|Rain)=0.6

**Task:** Find Ï€ = (Ï€_s, Ï€_r) such that Ï€ = Ï€ Â· T and Ï€_s + Ï€_r = 1.

**Answer (derivation):**

```
Ï€_s = 0.8 Ï€_s + 0.4 Ï€_r
Ï€_s - 0.8 Ï€_s = 0.4 Ï€_r
0.2 Ï€_s = 0.4 Ï€_r  â‡’  Ï€_s = 2 Ï€_r

Ï€_s + Ï€_r = 1  â‡’  2Ï€_r + Ï€_r = 1  â‡’  3Ï€_r = 1  â‡’  Ï€_r = 1/3
Ï€_s = 2/3

Ï€ = (2/3, 1/3)
```

**Quick checks:**

```
Flow Sunâ†’Rain:  (2/3)*0.2 = 2/15
Flow Rainâ†’Sun:  (1/3)*0.4 = 2/15   (balanced)

Ï€Â·T:
Sun' = (2/3)*0.8 + (1/3)*0.4 = 2/3
Rain'= (2/3)*0.2 + (1/3)*0.6 = 1/3  âœ”
```

---

## Q3 â€” Markov order

**Task:** Explain first- vs second-order Markov.

**Answer:**

```
First-order:  P(X_t | X_0: t-1) = P(X_t | X_{t-1})      (depends on 1 prev state)
Second-order: P(X_t | X_0: t-1) = P(X_t | X_{t-2}, X_{t-1}) (depends on 2 prev states)
```

---

## Q4 â€” Stationarity assumption

**Task:** What does â€œstationarityâ€ mean here?

**Answer:**

```
Transition P(X_t | X_{t-1}) and sensor P(E_t | X_t) are time-invariant (same CPTs for all t).
(Not about tâ†’âˆ; itâ€™s about parameters not changing across time steps.)
```

---

## Q5 â€” Two-step transition probability

**Given:** initial P(Xâ‚=Sun)=0.5, transitions:

* P(Rain|Sun)=0.2
* P(Rain|Rain)=0.6

**Task:** Compute P(Xâ‚‚=Rain).

**Answer:**

```
P(Xâ‚‚=Rain) = P(Sun)*P(Rain|Sun) + P(Rain)*P(Rain|Rain)
            = 0.5*0.2 + 0.5*0.6
            = 0.1 + 0.3
            = 0.4
```

---

## Q6 â€” Why initial influence fades

**Task:** Explain the intuition.

**Answer:**

```
Repeated application of the transition matrix â€œmixesâ€ the distribution.
For regular chains, P(X_t) â†’ stationary Ï€ regardless of the start.
Memory of the initial state decays; long-run behavior is governed by T.
```

---

## Q7 â€” Transition diagram (text)

**Given:**

* P(Sick|Healthy)=0.1
* P(Healthy|Sick)=0.4
  (So P(Healthy|Healthy)=0.9, P(Sick|Sick)=0.6)

**Answer (ASCII):**

```
Healthy --0.1--> Sick
   ^               |
   |               v
   <--0.4------- Healthy
```

---

Here is the **Section B (HMM + DBN)** question-and-answer set, formatted cleanly for **GitHub README**.
Readable, no LaTeX reliance, only Markdown + code blocks + simple HTML.

---

# âœ… Section B â€” HMMs & DBNs (Q8â€“Q25)

---

## **Q8 â€” HMM filtering (unnormalized first step)**

**Given:**
States: Rain, Sun
Evidence: `U` (Umbrella)
Transitions:

* P(Rain|Rain)=0.7, P(Sun|Rain)=0.3
* P(Rain|Sun)=0.2, P(Sun|Sun)=0.8
  Emissions:
* P(U|Rain)=0.9
* P(U|Sun)=0.2
  Initial: P(Rain)=0.5, P(Sun)=0.5

**Task:** Compute unnormalized P(Xâ‚ | eâ‚ = U).

**Answer:**

```
P(Rain, U) = 0.5 * 0.9 = 0.45
P(Sun,  U) = 0.5 * 0.2 = 0.10

Unnormalized posterior = (0.45, 0.10)
```

---

## **Q9 â€” Sensor Markov assumption**

**Answer:**

```
P(E_t | X_0:t, E_0:t-1) = P(E_t | X_t)

Current evidence depends ONLY on the current hidden state, nothing else.
```

---

## **Q10 â€” Hidden vs observed variables in HMM**

**Answer:**

```
Hidden (X_t): unobserved true state (e.g., weather, robot position).
Observed (E_t): noisy evidence emitted by the state (e.g., umbrella, sensor reading).
```

---

## **Q11 â€” Prediction step**

**Given belief:**
B(Xâ‚œ) = [Sun=0.6, Rain=0.4]

**Transition matrix T:**

```
[ 0.7  0.3 ]   # Sun->(Sun,Rain)
[ 0.4  0.6 ]   # Rain->(Sun,Rain)
```

**Task:** Predict B(Xâ‚œâ‚Šâ‚) before evidence.

**Answer:**

```
P(Sun')  = 0.6*0.7 + 0.4*0.4 = 0.42 + 0.16 = 0.58
P(Rain') = 0.6*0.3 + 0.4*0.6 = 0.18 + 0.24 = 0.42

Predicted = (0.58, 0.42)
```

---

## **Q12 â€” Why normalization matters**

**Answer:**

```
After multiplying by evidence likelihoods, values are NOT probabilities anymore.
Without normalization, P(X_t | e_1:t) won't sum to 1.
Belief state becomes invalid; future updates produce wrong results.
```

---

## **Q13 â€” Filtering recursion**

**Answer (standard forward formula):**

```
P(X_t | e_1:t) âˆ P(e_t | X_t) * Î£_{x_{t-1}} [ P(X_t | X_{t-1}) * P(X_{t-1} | e_1:t-1) ]
```

---

## **Q14 â€” Two-step filtering structure**

**Answer:**

```
1. Elapse time:
   Prediction:   P(X_t) = Î£_x P(X_t | X_{t-1}) * P(X_{t-1}|e_1:t-1)

2. Observation:
   Correction:   Multiply by P(e_t | X_t) and normalize.
```

---

## **Q15 â€” Define filtering / prediction / smoothing**

**Answer:**

```
Filtering:  P(X_t | e_1:t)
            Uses all evidence up to t.

Prediction: P(X_{t+k} | e_1:t)
            No evidence after t.

Smoothing:  P(X_k | e_1:t)  for k < t
            Uses future evidence as well.
```

---

## **Q16 â€” Most likely explanation**

**Task:** For evidence sequence eâ‚:â‚ƒ = {U, U, Â¬U}, what algorithm finds the single best hidden-state path?

**Answer:**

```
The Viterbi algorithm.
Returns argmax over the entire sequence X_1:3.
```

---

## **Q17 â€” When HMM reduces to Markov chain**

**Answer:**

```
When observations are perfectly informative (deterministic), or when you ignore them entirely.
Then hidden states become directly observed â†’ simple Markov chain.
```

---

## **Q18 â€” Correcting misconception**

**Claim:** â€œE_t depends on E_{t-1}.â€

**Answer:**

```
Wrong. Evidence nodes have no edges between them.
Correct: E_t depends ONLY on X_t.
```

---

## **Q19 â€” Robot tracking state & observation**

**Answer:**

```
State X_t      = robot's true position on grid (hidden)
Observation E_t = sensor readings (range, lidar, distance)
```

---

## **Q20 â€” Unnormalized posterior update**

**Given:**
P(Xâ‚œ | eâ‚:â‚œ) = [0.3, 0.7]
Next evidence favors state1 by factor 2: L = [2, 1]

**Answer:**

```
Unnormalized = [0.3*2, 0.7*1] = (0.6, 0.7)
```

---

## **Q21 â€” How DBN generalizes HMM**

**Answer:**

```
HMM = 1 hidden variable X_t and 1 evidence E_t per slice.
DBN = multiple variables per time slice, arbitrary dependencies.

Example extra variables: robot orientation, velocity, multiple sensors.
```

---

## **Q22 â€” Computational issue with unrolling DBNs**

**Answer:**

```
Exact inference cost grows with number of time steps t.
Graph becomes huge; variable elimination becomes intractable.
```

---

## **Q23 â€” Roll-up filtering**

**Answer:**

```
Add slice t+1, incorporate transitions, then sum-out (eliminate) slice t.
Keeps constant-size belief representation over time.
```

---

## **Q24 â€” Why likelihood weighting fails in long DBNs**

**Answer:**

```
Weight of samples that match evidence decays exponentially with time.
Most samples become effectively weight = 0 â†’ degeneracy.
```

---

## **Q25 â€” Four inference tasks with examples**

**Answer:**

```
1. Filtering     â†’ tracking a speaker's mouth position live.
2. Prediction    â†’ forecasting tomorrow's weather.
3. Smoothing     â†’ correcting yesterdayâ€™s robot position using later data.
4. MLE / Viterbi â†’ finding spoken word sequence in speech recognition.
```

---

Perfect â€” now Iâ€™ve cross-analyzed all your uploaded slides:

* **CSCI 390 â€“ Temporal Probability Models.pdf** (HMM/DBN)
* **chapter18.pdf + chapter18_part2.pdf** (Learning Agents, Inductive Learning, Ockhamâ€™s Razor, Decision Trees, Entropy, Information Gain, Pruning, Overfitting & Validation).

These cover most of your announced **quiz topics**.
Below is the mapping + carved-out quiz sets.

---

# ğŸ§­ Topic Map

| Exam Topic                                        | Covered in                        | Core Ideas                                                       |
| ------------------------------------------------- | --------------------------------- | ---------------------------------------------------------------- |
| **Learning Agent**                                | ch 18 (Â§1â€“3)                      | performance element, learning element, critic, problem generator |
| **Ockhamâ€™s Razor**                                | ch 18 p 12 & slides 15 â€“ 16       | simplicity vs consistency                                        |
| **Decision Trees & Pruning**                      | ch 18 p 22 â€“ 31 + part2 (Pruning) | entropy, information gain, Chi-square, early stopping            |
| **Overfitting / Validation**                      | part2 (Overfitting)               | training vs test error, cross-validation                         |
| **Parametric vs Non-parametric (LDA, KDE / GMM)** | not in these, in other slides     | â€”                                                                |
| **HMM / DBN**                                     | Temporal Prob Models              | already done earlier                                             |

---



# ğŸ§© Topic 1 â€“ Learning Agent Model

```
1. What are the four components of a learning agent?
â†’ Performance element, Learning element, Critic, Problem generator.

2. Role of performance element?
â†’ Chooses actions based on current percepts and internal state.

3. Role of learning element?
â†’ Improves the performance element using feedback from the critic.

4. Role of critic?
â†’ Provides performance feedback by comparing behavior to a standard.

5. Role of problem generator?
â†’ Proposes exploratory actions to discover new, informative experiences.

6. Difference between supervised and reinforcement feedback?
â†’ Supervised gives correct labels each instance; RL gives occasional rewards.

7. How does learning modify the agent?
â†’ Adjusts decision mechanisms to improve future performance.

8. Why learning useful for system design?
â†’ Lets environment teach the agent instead of hard-coding rules.

9. What dictates the design of the learning element?
â†’ Type of performance element, component to learn, representation, feedback type.

10. Example mapping:
â†’ Utility-based agent learns perceptâ†’action function; feedback = reward signal.
```

---

# ğŸª¶ Topic 2 â€“ Ockhamâ€™s Razor and Inductive Learning

```
1. Quote meaning:
   â€œEntities should not be multiplied unnecessarilyâ€ â†’ prefer simpler models.

2. Why simplicity helps?
   â†’ Reduces risk of overfitting; better generalization.

3. In inductive learning, what is f(x)?
   â†’ Target function mapping inputs to outputs.

4. What is hypothesis h?
   â†’ Learnerâ€™s approximation of f using training data.

5. When is h consistent?
   â†’ When it agrees with all training examples.

6. Relation of Ockhamâ€™s Razor to decision trees?
   â†’ Prefer smaller trees consistent with data.

7. What is biasâ€“variance intuition here?
   â†’ Simpler models have higher bias, lower variance; complex opposite.

8. Give an example of inductive bias.
   â†’ â€œThe simplest consistent hypothesis is best.â€

9. What assumptions real inductive learning simplifies?
   â†’ Deterministic f, observable inputs, given examples, agent wants to learn f.

10. Why curve-fitting demonstrates Ockhamâ€™s Razor?
   â†’ A smoother curve explaining data points is preferred to jagged overfit.
```

---

# ğŸŒ³ Topic 3 â€“ Decision Tree Learning & Information Gain

```
1. Aim:
   â†’ Find the smallest tree consistent with training examples.

2. Recursion base cases:
   â€¢ All examples same class â†’ return class.
   â€¢ No examples â†’ return default.
   â€¢ No attributes â†’ return mode class.

3. Attribute selection criterion?
   â†’ Information Gain = H(parent) âˆ’ Remainder(A).

4. Entropy formula:
   H(p, n) = âˆ’p/(p+n) logâ‚‚(p/(p+n)) âˆ’ n/(p+n) logâ‚‚(n/(p+n))

5. Remainder(A):
   Î£_i (p_i + n_i)/(p+n) Ã— H(p_i, n_i)

6. Choose attribute with smallest Remainder â†’ highest InfoGain.

7. Example: Patrons? vs Type? (Restaurant)
   Patrons? Gain â‰ˆ 0.54 bits > Type? 0 bits â†’ choose Patrons?.

8. What happens with many attributes?
   â†’ Larger hypothesis space â†’ risk of overfitting.

9. Decision tree expressiveness?
   â†’ Can represent any Boolean function (2â½Â²â¿â¾ possible trees).

10. Why prefer compact tree?
   â†’ Simpler â‡’ better generalization (Ockhamâ€™s Razor).
```

---

# âœ‚ï¸ Topic 4 â€“ Decision Tree Pruning (Chi-Square / Validation)

```
1. Purpose of pruning?
   â†’ Reduce overfitting by removing statistically insignificant splits.

2. Two strategies:
   â†’ Post-pruning (generate then prune) vs Early stopping (halt before split).

3. Chi-square test null hypothesis?
   â†’ â€œNo real patternâ€ between attribute and class.

4. Compute Ï‡Â²:
   Ï‡Â² = Î£_k ( (observed_k âˆ’ expected_k)Â² / expected_k )

5. Degrees of freedom (df):
   df = (#positive + #negative classes âˆ’ 1) Ã— (#attribute values âˆ’ 1)

6. Example threshold:
   â†’ df=3 â†’ critical 7.82 at 5%; if Ï‡Â² > 7.82, keep split; else prune.

7. Why not just remove low-gain attributes directly?
   â†’ Combination of low-gain attributes may still classify jointly (e.g., XOR).

8. Effect of noise:
   â†’ Pruning helps by ignoring spurious correlations.

9. What is Early stopping criterion?
   â†’ Stop if split doesnâ€™t improve InfoGain significantly on validation set.

10. Trade-off:
   â†’ Too much pruning = underfit; too little = overfit.
```

---

# ğŸ“ˆ Topic 5 â€“ Overfitting & Model Generalization / Validation

```
1. Define overfitting:
   â†’ Model fits training noise; performs poorly on unseen data.

2. Define underfitting:
   â†’ Model too simple to capture data patterns.

3. Typical symptom:
   â†’ Training accuracy â†‘, test accuracy â†“ as epochs continue.

4. How to visualize?
   â†’ Plot train/test loss vs epochs; test loss bottoms then rises.

5. Cross-validation (k-fold):
   â†’ Split dataset into k parts; train k times each leaving one fold out.

6. Purpose of validation set:
   â†’ Tune hyperparameters and detect overfitting before test phase.

7. How does hypothesis space size affect overfitting?
   â†’ Larger space â†’ higher variance â†’ greater risk.

8. Give numeric example:
   â†’ Acc(train)=99%, Acc(test)=60% â†’ classic overfit.

9. How to combat overfitting?
   â†’ Simplify model, regularization, pruning, or add data.

10. Why Ockhamâ€™s Razor applies again?
   â†’ Simpler model less likely to memorize noise â‡’ better generalization.
```

---

# ğŸ§  Topic 6 â€“ Temporal Models (HMM / DBN Recap)

(Already derived from previous â€œSection Bâ€)

```
1. Markov assumption â†’ P(X_t | X_0:tâˆ’1) = P(X_t | X_{tâˆ’1})
2. Sensor Markov â†’ P(E_t | X_0:t, E_0:tâˆ’1) = P(E_t | X_t)
3. Filtering â†’ update belief with evidence e_t
4. Prediction â†’ project belief forward without new evidence
5. Smoothing â†’ re-estimate past given future data
6. Forward algorithm â†’ Elapse + Observe + Normalize
7. Stationary distribution â†’ Ï€ = Ï€T
8. HMM components â†’ (Ï€â‚, A, B)
9. Viterbi = most likely state sequence
10. DBN = multi-variable generalization of HMM.
```

---



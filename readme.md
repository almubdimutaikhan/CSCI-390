# ğŸ“˜ Stationary Distribution (Steady-State Probabilities)

### ğŸ”¹ Definition
The **stationary distribution** is a probability vector **Ï€ = [P<sub>âˆ</sub>(A), P<sub>âˆ</sub>(B), P<sub>âˆ</sub>(C)]**  
such that the system stays unchanged after transitions:

**Ï€ = Ï€ T**

---

### ğŸ”¹ Core Equation
For every state *i*:
> P(i) = Î£ P(j) Â· P(j â†’ i)

and  
> P(A) + P(B) + P(C) = 1  

---

### ğŸ”¹ Steps to Find Ï€

1. **Write transition matrix T** (rows = from-state, columns = to-state).  
2. **Form balance equations**  
   - P(A) = P(A) P(Aâ†’A) + P(B) P(Bâ†’A) + P(C) P(Câ†’A)  
   - P(B) = P(A) P(Aâ†’B) + P(B) P(Bâ†’B) + P(C) P(Câ†’B)  
   - P(C) = P(A) P(Aâ†’C) + P(B) P(Bâ†’C) + P(C) P(Câ†’C)
3. **Add normalization:** P(A)+P(B)+P(C)=1  
4. **Solve** the linear system for P(A), P(B), P(C).

---

### ğŸ”¹ Notes
- Represents the **long-run fraction of time** spent in each state.  
- Exists if the chain is **ergodic** (connected + aperiodic).  
- Equivalent to the **eigenvector of Táµ€** with eigenvalue 1.  

---

### ğŸ§  Example
If P(A)=P(B) and P(C)=0.5 P(A):  
â†’ 2 P(A)+0.5 P(A)=1 â†’ P(A)=0.4  
âœ… Pâˆ(A)=0.4, Pâˆ(B)=0.4, Pâˆ(C)=0.2



## ğŸŒ³ Decision Tree â€” Information Gain (Quick Revision)

### 1ï¸âƒ£ Step 1 â€” Compute Overall Entropy

Target variable (class) = Z

$$
H(Z) = -\sum_i p_i \log_2 p_i
$$

* Measures **uncertainty** in Z.
* Example: 5 True, 5 False â†’ (H(Z)=1).

---

### 2ï¸âƒ£ Step 2 â€” Compute Conditional Entropy for each Attribute

For each attribute (A):

* Split the data by Aâ€™s values (e.g., A=0 and A=1).
* For each subset, compute entropy of Z inside it.

$$
H(Z|A) = \sum_{v \in Values(A)} P(A=v) , H(Z|A=v)
$$

where
$$
H(Z|A=v) = -\sum_i P(Z=i | A=v) \log_2 P(Z=i | A=v)
$$

âœ… Weighted average:
Each subset entropy is weighted by its proportion in total data.

---

### 3ï¸âƒ£ Step 3 â€” Compute Information Gain

$$
IG(Z, A) = H(Z) - H(Z|A)
$$

* The **higher the IG**, the more that attribute reduces uncertainty.
* Choose the **attribute with the largest IG** as the root.

---

### 4ï¸âƒ£ Step 4 â€” Recursive Split

* Repeat steps 1â€“3 **within each branch**, using remaining attributes.
* Stop when:

  * All examples in a node have the same class, or
  * No attributes left (or IG = 0).

---

### ğŸ§  Quick Intuition

| Term                 | Meaning                                               |
| -------------------- | ----------------------------------------------------- |
| **Entropy**          | How mixed the classes are                             |
| **Information Gain** | How much splitting reduces uncertainty                |
| **High IG**          | Attribute gives strong signal about target            |
| **Low IG (â‰ˆ0)**      | Attribute doesnâ€™t help â€” same mixture after splitting |

---

### âš¡ Formula Summary

$$
\begin{aligned}
H(Z) &= -\sum p_i \log_2 p_i \
H(Z|A) &= \sum_v P(A=v) H(Z|A=v) \
IG(Z,A) &= H(Z) - H(Z|A)
\end{aligned}
$$

---

### âœ… Tip for Fast Exam Solving

1. Count positives and negatives for Z.
2. Compute base entropy.
3. Split by each attribute.
4. Compute branch entropies â†’ weighted sum.
5. Pick the one with **max IG**.

---



# ğŸ¤– AI Quiz 2 â€” Summary Notes

## 1ï¸âƒ£ Bayesian Networks & Exact Inference

**Core ideas**

* Conditional independence; chain rule factorization of joint $P(X_1,\dots,X_n)=\prod_i P(X_i|\text{Parents}(X_i))$.
* **Query = wanted vars; Evidence = known values; Hidden = others â†’ eliminate.**
* **Variable elimination**:

  1. Fix evidence.
  2. Multiply factors that share a var.
  3. Sum out that var.
  4. Repeat for hidden vars only.
* **Order choice:** use *min-fill* or *min-degree* to keep factors small.
* **Enumeration vs Elimination:** both exact; elimination is faster.
* **Sampling:**

  * *Prior* â€“ top-down sampling (no evidence).
  * *Rejection* â€“ discard inconsistent samples.
  * *Likelihood Weighting* â€“ keep all, weight by evidence likelihood.

---

## 2ï¸âƒ£ Temporal / Hidden Markov Models

* **Markov assumption:** $P(X_t|X_{t-1},â€¦,X_0)=P(X_t|X_{t-1})$.
* **Filtering:** alternate

  * *Elapse time* â€“ predict next state.
  * *Observe evidence* â€“ update with $P(E_t|X_t)$.
* **Forward algorithm:** recursively apply Elapseâ†’Observe to maintain belief state.
* **Stationary distribution:** solve $P_\infty=P_\infty T$ (sum = 1).

---

## 3ï¸âƒ£ Decision Trees / Statistical Learning

* **Entropy:** $H(S)=-\sum p_i\log_2p_i$.
* **Conditional entropy:** $H(S|A)=\sum_v P(A=v)H(S|A=v)$.
* **Information Gain:** $IG(S,A)=H(S)-H(S|A)$.
* **Build tree:** choose attr. with max IG â†’ recurse.
* **Stop:** pure node or IG = 0.
* **Overfitting:** deeper = lower bias / higher variance.
* **Statistical learning:**

  * Likelihood $L(\theta|D)=P(D|\theta)$.
  * **MLE:** maximize $L$.
  * **MAP:** maximize $L\times P(\theta)$.
  * Posterior $P(\theta|D)\propto P(D|\theta)P(\theta)$.

---

## 4ï¸âƒ£ Rational Decision Theory

### ğŸ² Lotteries

$L=[p,A;(1-p),B]$ â†’ outcome A with p, B otherwise.
**Expected Utility:** $EU(L)=\sum_i p_iU(x_i)$.
**MEU rule:** pick action with max EU.

### ğŸ’° Utility of Money

* Money â‰  utility â†’ diminishing returns â†’ **risk-averse** (concave U).
* **Risk types:**

  | Shape   | Behavior     | Example U(x) |
  | ------- | ------------ | ------------ |
  | Concave | Risk-averse  | log x, âˆšx    |
  | Linear  | Risk-neutral | x            |
  | Convex  | Risk-seeking | xÂ²           |
* **Certainty Equivalent:** sure x s.t. U(x)=EU(L).
* **Affine invariance:** Uâ€²=kâ‚U+kâ‚‚ (kâ‚>0) â‡’ same decisions.

### ğŸ§© Rationality Axioms

1. **Completeness:** can compare any two outcomes.
2. **Transitivity:** if A > B and B > C â‡’ A > C.
3. **Continuity:** mixtures possible.
4. **Independence:** common components donâ€™t flip preference.
   Violating transitivity â‡’ *money-pump* paradox.

### ğŸ§® Multiattribute Utility

* $U(x_1,â€¦,x_n)$ when outcomes have many attributes.
* Use **preference independence** to simplify:

  * Additive form $U=\sum w_i u_i(x_i)$
  * Multiplicative form $U=\prod (1+k,u_i)$.

### ğŸ“ˆ Stochastic Dominance

* **First-order:** one distributionâ€™s CDF always below another â†’ dominates.
* If A dominates B, every rational (monotonic U) prefers A.
* Used for comparing uncertain options or causal influences ( + / â€“ arrows ).

---

## 5ï¸âƒ£ Summary Mind-Map

* **Probability model â†’ Inference:** enumerate / eliminate / sample.
* **Temporal model â†’ Belief update:** forward filtering.
* **Learning â†’ Fit parameters:** MLE/MAP.
* **Decision â†’ Choose action:** maximize expected utility.

---

### âœ… Quick exam steps

1. Identify **given, query, evidence**.
2. For Bayes net â†’ eliminate hidden only.
3. For DT â†’ compute H & IG, pick max.
4. For HMM â†’ apply Elapseâ†’Observeâ†’Normalize.
5. For decisions â†’ compute EU, compare.
6. For learning â†’ compute L, find Î¸Ì‚ (MLE / MAP).

---



# ğŸ§  ML & Decision Tree â€” Quick Review Notes

## ğŸŒ³ CART (Classification and Regression Trees)

* Binary tree: each node splits data into **two subsets** $Dâ‚ âˆ© Dâ‚‚ = âˆ…, Dâ‚ âˆª Dâ‚‚ = D$.
* Split chosen by **one variable + threshold**; repeated recursively until a **leaf** (terminal node).
* Same attribute can appear multiple times in deeper nodes.
* Works for:

  * **Classification** â†’ uses **Entropy** or **Gini index**
  * **Regression** â†’ minimizes **SSE (Sum of Squared Errors)**

### ğŸ” Greedy Split Strategy

* Choose the split with **maximum information gain** (most purity).
* **Recursive Binary Splitting** stops when nodes are pure or cannot split further.
* **Purity:** node has only one class of samples

### ğŸ§® Impurity Measures

| Measure        | Formula Idea           | Range | Interpretation                    |
| -------------- | ---------------------- | ----- | --------------------------------- |
| **Entropy**    | $-\sum p_i \log_2 p_i$ | 0â€“1   | 0 = pure, 1 = mixed               |
| **Gini Index** | $1 - \sum p_i^2$       | 0â€“0.5 | 0 = pure, 0.5 = worst for 2-class |

---

## âš ï¸ Why a Single Tree Fails

* Overfits â€” too sensitive to outliers and imbalanced data.
* High **variance**: small training changes â†’ large structure changes.
* Poor generalization to test data

---

## ğŸ¤ Ensemble Methods

### ğŸªµ Bagging (Bootstrap Aggregating)

* Train **many models** on different **bootstrapped samples** (sampling *with replacement*).
* Each model votes â†’ **majority (classification)** or **average (regression)**.
* Goal: reduce **variance**, smooth boundaries, and reduce overfitting.
* Example: 30 trees â†’ smoother, more stable than 1 tree (Train acc 1.0, Test acc â†‘0.778â†’0.822).

### ğŸŒ² Random Forest

* **Bagging + Random Attribute Splits**

  * Bootstrapped samples + at each node, pick a **random subset of features** to split.
* Encourages **diversity among trees**, lowering correlation between them.
* Still aggregates via majority/average voting.

**Summary Difference:**

| Method        | Sampling             | Feature Randomness              | Goal                          |
| ------------- | -------------------- | ------------------------------- | ----------------------------- |
| Bagging       | Bootstrapped samples | All features used               | Reduce variance               |
| Random Forest | Bootstrapped samples | Random feature subset per split | Reduce variance + correlation |

---

## âš™ï¸ Linear vs Non-Linear Classifiers

* **Linear:** decision boundary is a straight line/plane.

  * $w^T x > b â†’ +$, else $â€“$.
  * Easy to interpret but limited flexibility.
* **Non-Linear:** use transformations (kernels, neural nets) to separate complex data.

  * Example: project to higher dimension where classes become linearly separable.
* **Projection matrix (w):** projects data to simpler decision space.

---

## ğŸ§© Parametric vs Non-Parametric

| Type               | Description                                 | Examples                        |
| ------------------ | ------------------------------------------- | ------------------------------- |
| **Parametric**     | Fixed #parameters; model form predetermined | Linear Regression, LDA, DNN     |
| **Non-Parametric** | Model complexity grows with data            | Decision Tree, KNN, SVM(kernel) |

---

## ğŸ§  KNN (K-Nearest Neighbors)

* **Instance-based**, non-parametric: no explicit model.
* Classify by majority vote of nearest K neighbors (distance-based).
* Sensitive to scale, irrelevant features, and high dimensions (curse of dimensionality).

---

## ğŸ” HMM vs MM (context for your quiz)

* **Markov Model (MM):** states observable, uses transition $P(W_t|W_{t-1})$.
* **Hidden Markov Model (HMM):** states hidden, uses both

  * Transition: $P(W_t|W_{t-1})$
  * Emission: $P(O_t|W_t)$.
* Solved via **Forward Algorithm** (predict â†’ update), e.g. $P(W_2|O_1,O_2)$ as in your weather example.

---

### ğŸ§® Forward Algorithm (Steps)

1. **Initialize:** prior $P(W_0)$
2. **Elapse (Predict):** multiply by transition matrix
3. **Observe (Update):** multiply element-wise by emission prob.
4. **Normalize**
   â†’ repeat for all observations.
   Final normalized vector = posterior probability of states.

---



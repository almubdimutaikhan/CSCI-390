# ğŸ“˜ Stationary Distribution (Steady-State Probabilities)

### ğŸ”¹ Definition
The **stationary distribution** is a probability vector **Ï€ = [P<sub>âˆ</sub>(A), P<sub>âˆ</sub>(B), P<sub>âˆ</sub>(C)]**  
such that the system stays unchanged after transitions:

**Ï€ = Ï€ T**

---

### ğŸ”¹ Core Equation
For every state *i*:
> P(i) = Î£ P(j) Â· P(j â†’ i)

and  
> P(A) + P(B) + P(C) = 1  

---

### ğŸ”¹ Steps to Find Ï€

1. **Write transition matrix T** (rows = from-state, columns = to-state).  
2. **Form balance equations**  
   - P(A) = P(A) P(Aâ†’A) + P(B) P(Bâ†’A) + P(C) P(Câ†’A)  
   - P(B) = P(A) P(Aâ†’B) + P(B) P(Bâ†’B) + P(C) P(Câ†’B)  
   - P(C) = P(A) P(Aâ†’C) + P(B) P(Bâ†’C) + P(C) P(Câ†’C)
3. **Add normalization:** P(A)+P(B)+P(C)=1  
4. **Solve** the linear system for P(A), P(B), P(C).

---

### ğŸ”¹ Notes
- Represents the **long-run fraction of time** spent in each state.  
- Exists if the chain is **ergodic** (connected + aperiodic).  
- Equivalent to the **eigenvector of Táµ€** with eigenvalue 1.  

---

### ğŸ§  Example
If P(A)=P(B) and P(C)=0.5 P(A):  
â†’ 2 P(A)+0.5 P(A)=1 â†’ P(A)=0.4  
âœ… Pâˆ(A)=0.4, Pâˆ(B)=0.4, Pâˆ(C)=0.2



## ğŸŒ³ Decision Tree â€” Information Gain (Quick Revision)

### 1ï¸âƒ£ Step 1 â€” Compute Overall Entropy

Target variable (class) = Z

[
H(Z) = -\sum_i p_i \log_2 p_i
]

* Measures **uncertainty** in Z.
* Example: 5 True, 5 False â†’ (H(Z)=1).

---

### 2ï¸âƒ£ Step 2 â€” Compute Conditional Entropy for each Attribute

For each attribute (A):

* Split the data by Aâ€™s values (e.g., A=0 and A=1).
* For each subset, compute entropy of Z inside it.

[
H(Z|A) = \sum_{v \in Values(A)} P(A=v) , H(Z|A=v)
]

where
[
H(Z|A=v) = -\sum_i P(Z=i | A=v) \log_2 P(Z=i | A=v)
]

âœ… Weighted average:
Each subset entropy is weighted by its proportion in total data.

---

### 3ï¸âƒ£ Step 3 â€” Compute Information Gain

[
IG(Z, A) = H(Z) - H(Z|A)
]

* The **higher the IG**, the more that attribute reduces uncertainty.
* Choose the **attribute with the largest IG** as the root.

---

### 4ï¸âƒ£ Step 4 â€” Recursive Split

* Repeat steps 1â€“3 **within each branch**, using remaining attributes.
* Stop when:

  * All examples in a node have the same class, or
  * No attributes left (or IG = 0).

---

### ğŸ§  Quick Intuition

| Term                 | Meaning                                               |
| -------------------- | ----------------------------------------------------- |
| **Entropy**          | How mixed the classes are                             |
| **Information Gain** | How much splitting reduces uncertainty                |
| **High IG**          | Attribute gives strong signal about target            |
| **Low IG (â‰ˆ0)**      | Attribute doesnâ€™t help â€” same mixture after splitting |

---

### âš¡ Formula Summary

[
\begin{aligned}
H(Z) &= -\sum p_i \log_2 p_i \
H(Z|A) &= \sum_v P(A=v) H(Z|A=v) \
IG(Z,A) &= H(Z) - H(Z|A)
\end{aligned}
]

---

### âœ… Tip for Fast Exam Solving

1. Count positives and negatives for Z.
2. Compute base entropy.
3. Split by each attribute.
4. Compute branch entropies â†’ weighted sum.
5. Pick the one with **max IG**.

---

